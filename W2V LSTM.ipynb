{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgensim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Word2Vec\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequence\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pad_sequences\n\u001B[1;32m----> 8\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_pickle\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mfinal_df.pkl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m X\u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenized\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     10\u001B[0m y\u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\pandas\\io\\pickle.py:208\u001B[0m, in \u001B[0;36mread_pickle\u001B[1;34m(filepath_or_buffer, compression, storage_options)\u001B[0m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m warnings\u001B[38;5;241m.\u001B[39mcatch_warnings(record\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    206\u001B[0m         \u001B[38;5;66;03m# We want to silence any warnings about, e.g. moved modules.\u001B[39;00m\n\u001B[0;32m    207\u001B[0m         warnings\u001B[38;5;241m.\u001B[39msimplefilter(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;167;01mWarning\u001B[39;00m)\n\u001B[1;32m--> 208\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhandles\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m excs_to_catch:\n\u001B[0;32m    210\u001B[0m     \u001B[38;5;66;03m# e.g.\u001B[39;00m\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001B[39;00m\n\u001B[0;32m    212\u001B[0m     \u001B[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001B[39;00m\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pc\u001B[38;5;241m.\u001B[39mload(handles\u001B[38;5;241m.\u001B[39mhandle, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\venv\\lib\\site-packages\\numpy\\core\\numeric.py:1869\u001B[0m, in \u001B[0;36m_frombuffer\u001B[1;34m(buf, dtype, shape, order)\u001B[0m\n\u001B[0;32m   1861\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1864\u001B[0m _fromfunction_with_like \u001B[38;5;241m=\u001B[39m array_function_dispatch(\n\u001B[0;32m   1865\u001B[0m     _fromfunction_dispatcher\n\u001B[0;32m   1866\u001B[0m )(fromfunction)\n\u001B[1;32m-> 1869\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_frombuffer\u001B[39m(buf, dtype, shape, order):\n\u001B[0;32m   1870\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m frombuffer(buf, dtype\u001B[38;5;241m=\u001B[39mdtype)\u001B[38;5;241m.\u001B[39mreshape(shape, order\u001B[38;5;241m=\u001B[39morder)\n\u001B[0;32m   1873\u001B[0m \u001B[38;5;129m@set_module\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   1874\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21misscalar\u001B[39m(element):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "df = pd.read_pickle('final_df.pkl')\n",
    "X= df['tokenized']\n",
    "y= df['sentiment']\n",
    "\n",
    "##0.2 so that training data is 80% and test data 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69, stratify = y)\n",
    "\n",
    "## Changing it to a list of lists as input to Word2Vec\n",
    "corpus = list(X_train)\n",
    "\n",
    "##Defining the size and running w2v\n",
    "size = 300\n",
    "w2v_model = Word2Vec(sentences=corpus, vector_size = size)\n",
    "word_vectors = w2v_model.wv\n",
    "\n",
    "##Tokenizing\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "dic_vocabulary = tokenizer.word_index\n",
    "\n",
    "maxlen = 100\n",
    "##Padding the sequences\n",
    "train_token_seq = tokenizer.texts_to_sequences(corpus)\n",
    "X_train_pad = pad_sequences(train_token_seq, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, size))\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  word_vectors[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "##Trying to see if it works\n",
    "word = \"nice\"\n",
    "print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
    "print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape,\n",
    "      \"|vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_14 (Embedding)    (None, 100, 300)          53060100  \n",
      "                                                                 \n",
      " lstm_16 (LSTM)              (None, 16)                20288     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,080,405\n",
      "Trainable params: 20,305\n",
      "Non-trainable params: 53,060,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##Making the framework for the neural network\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=embeddings.shape[0], output_dim=embeddings.shape[1],\n",
    "                    weights=[embeddings], input_length=X_train_pad.shape[1], trainable=False, input_shape=(maxlen,)))\n",
    "model.add(LSTM(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=[get_f1])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5250/5250 [==============================] - 140s 26ms/step - loss: 0.4903 - get_f1: 0.8884\n",
      "Epoch 2/5\n",
      "5250/5250 [==============================] - 143s 27ms/step - loss: 0.3519 - get_f1: 0.9171\n",
      "Epoch 3/5\n",
      "5250/5250 [==============================] - 145s 28ms/step - loss: 0.3300 - get_f1: 0.9221\n",
      "Epoch 4/5\n",
      "5250/5250 [==============================] - 146s 28ms/step - loss: 0.3191 - get_f1: 0.9250\n",
      "Epoch 5/5\n",
      "5250/5250 [==============================] - 145s 28ms/step - loss: 0.3087 - get_f1: 0.9283\n"
     ]
    }
   ],
   "source": [
    "##Defining class weight as positive class is twice as big as the negative class\n",
    "class_weight = {0: 2.,\n",
    "                1: 1.}\n",
    "lstm_model = model.fit(X_train_pad, y_train, batch_size=64, epochs=5, class_weight=class_weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lstm_model.history['get_f1'])\n",
    "plt.plot(lstm_model.history['val_get_f1'])\n",
    "plt.title('model F1 score')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(lstm_model.history['loss'])\n",
    "plt.plot(lstm_model.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "##Preprocessin the test set\n",
    "corpus2 = list(X_test)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "X_test_seq = tokenizer.texts_to_sequences(corpus2)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen, padding=\"post\", truncating=\"post\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2625/2625 [==============================] - 29s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_pad)\n",
    "lstm_predictions = list(map(lambda x: 0 if x<0.5 else 1, predictions))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9205916805143153"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, lstm_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.9577169415632262, 0.886237281023812)"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score(y_test, lstm_predictions), recall_score(y_test, lstm_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[24560,  2238],\n       [ 6507, 50691]], dtype=int64)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, lstm_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
