{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dic[word]: 209 |idx\n",
      "embeddings[idx]: (300,) |vector\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "df = pd.read_pickle('final_df.pkl')\n",
    "X= df['tokenized']\n",
    "y= df['sentiment']\n",
    "\n",
    "##0.2 so that training data is 80% and test data 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69, stratify = y)\n",
    "\n",
    "## Changing it to a list of lists as input to Word2Vec\n",
    "corpus = list(X_train)\n",
    "\n",
    "##Defining the size and running w2v\n",
    "size = 300\n",
    "w2v_model = Word2Vec(sentences=corpus, vector_size = size, sg=1)\n",
    "word_vectors = w2v_model.wv\n",
    "\n",
    "##Tokenizing\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "dic_vocabulary = tokenizer.word_index\n",
    "\n",
    "maxlen = 100\n",
    "##Padding the sequences\n",
    "train_token_seq = tokenizer.texts_to_sequences(corpus)\n",
    "X_train_pad = pad_sequences(train_token_seq, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "## start the matrix (length of vocabulary x vector size) with all 0s\n",
    "embeddings = np.zeros((len(dic_vocabulary)+1, size))\n",
    "for word,idx in dic_vocabulary.items():\n",
    "    ## update the row with vector\n",
    "    try:\n",
    "        embeddings[idx] =  word_vectors[word]\n",
    "    ## if word not in model then skip and the row stays all 0s\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "##Trying to see if it works\n",
    "word = \"nice\"\n",
    "print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\n",
    "print(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape,\n",
    "      \"|vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 300)          53060100  \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 100, 64)           70272     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 32)                9408      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,139,813\n",
      "Trainable params: 79,713\n",
      "Non-trainable params: 53,060,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##Making the framework for the neural network\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, GRU, Dropout, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=embeddings.shape[0], output_dim=embeddings.shape[1],\n",
    "                    weights=[embeddings], input_length=X_train_pad.shape[1], trainable=False, input_shape=(maxlen,)))\n",
    "model.add(GRU(64, activation='relu', return_sequences=True))\n",
    "model.add(GRU(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics='accuracy')\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5250/5250 [==============================] - 309s 59ms/step - loss: 0.2704 - accuracy: 0.8873\n",
      "Epoch 2/5\n",
      "5250/5250 [==============================] - 311s 59ms/step - loss: 0.2072 - accuracy: 0.9166\n",
      "Epoch 3/5\n",
      "5250/5250 [==============================] - 320s 61ms/step - loss: 0.1911 - accuracy: 0.9243\n",
      "Epoch 4/5\n",
      "5250/5250 [==============================] - 323s 61ms/step - loss: 0.1795 - accuracy: 0.9291\n",
      "Epoch 5/5\n",
      "5250/5250 [==============================] - 319s 61ms/step - loss: 0.1700 - accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "##Defining class weight as positive class is twice as big as the negative class\n",
    "\n",
    "gru_model = model.fit(X_train_pad, y_train, batch_size=64, epochs=5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(gru_model.history['get_f1'])\n",
    "plt.plot(gru_model.history['val_get_f1'])\n",
    "plt.title('model F1 score')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(gru_model.history['loss'])\n",
    "plt.plot(gru_model.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "##Preprocessin the test set\n",
    "corpus2 = list(X_test)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "X_test_seq = tokenizer.texts_to_sequences(corpus2)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=100, padding=\"post\", truncating=\"post\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2625/2625 [==============================] - 51s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_pad)\n",
    "gru_predictions = list(map(lambda x: 0 if x<0.5 else 1, predictions))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9463133300327453"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, gru_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.9403103585175979, 0.9523934403300814)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score(y_test, gru_predictions), recall_score(y_test, gru_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[23340,  3458],\n       [ 2723, 54475]], dtype=int64)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, gru_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
